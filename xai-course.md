
## Course on explainable machine learning (XAI)



## Course abstract

Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manneris a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact

is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? In this course, we focus specifically on data-driven methods – machine learning (ML) and pattern recognition models in particular – so as to survey and distill the results and observations from the literature.

The purpose of this course can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders

in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature, or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we aim to help industry practitioners understand the field of explainable machine learning better and apply the right tools. Our course work builds a narrative around a putative data scientist, and discusses how she might go about explaining her models by asking the right questions.

From an organization viewpoint, after motivating the area broadly, we turn to technical insights, which includes three frameworks: a taxonomic framework provides an overview of explainable ML, and the other two frameworks further study so-called transparent models vs opaque models in that taxonomy. The latter then requires model specific or model agnostic post-hoc explainability approaches, which have their individual limitations and strengths that are also discussed. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions. In terms of exercises, we will focus on 6 popular techniques and try to understand how to use these explainability techniques with coding problems for publicly available datasets.

## Who is this for

This online credit-bearing course is aimed at aspiring data scientists and software engineers, who have technical backgrounds and roles in organisations involving or related to machine learning and artificial intelligence. The course helps them understand and apply innovative and state-of-the-art techniques on explaining machine learning models so as to lead and manage the use of machine learning in their work place. Explainability will likely be a key component for encouraging major organisational changes via the wider deployment of artificial intelligence technologies, and this course will provide strong foundations for navigating and critiquing developments in the area.

## Course Delivery Information

Start Date: 01 June, 2021

Course Duration: 3 months (the final month for assignments only)

Method of Assessment: Coursework 100% (2 coding assignments and a final coding project)

Level:  This is an intermediate Masters-level course (SCQF Level 11). It develops your skills and/or provides a broad understanding of the subject in some detail - some foundational knowledge is required. Please see the entry requirements for further details. Masters-level courses are relatively intensive and require independent learning, critical thinking, analysis and reflection.
