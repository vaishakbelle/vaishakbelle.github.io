---
layout: page
title: Belle Lab
permalink: /lab/
---


The Lab carries out research in **artificial intelligence**, by unifying **learning** and **logic**, with a recent emphasis on *explainability* and *ethics*.

<img src="/uni.png" width="450"> 

We are motivated by the need to augment _learning_ and _perception_ with _high-level structured, commonsensical knowledge_, to enable systems to learn faster and more accurate models of the world. We are interested in developing computational frameworks that are able to _explain their decisions, modular, re-usable_, and _robust_ to variations in problem description. A non-exhaustive list of topics include:

*   probabilistic and statistical knowledge bases
*   ethics and explainability in AI 
*   exact and approximate probabilistic inference
*   statistical relational learning and causality
*   unifying deep learning and probabilistic learning methods
*   probabilistic programming
*   numerical optimization
*   automated planning and high-level programming
*   reinforcement learning and learning for automated planning
*   cognitive robotics
*   automated reasoning
*   modal logics (knowledge, action, belief)
*   multi-agent systems and epistemic planning
*   integrating causality and learning 

For example, our recent work has touched upon: 

*   [morality in machine learning systems](https://arxiv.org/pdf/1810.03736) 
*   [tractable learning with relational logic](/attachments/pacfol.pdf) 
*   [deep tractable probabilistic generative models](https://arxiv.org/pdf/1807.05464) 
*   [learning with missing data](https://arxiv.org/pdf/1901.05847)
*   [program learning for explainability](/attachments/ilp2019.pdf) 
*   [implementing fairness](https://arxiv.org/abs/1905.07026)
*   [model abstraction for explainability](https://arxiv.org/pdf/1810.02434) 
*   [strategies for interpretable & responsible AI](/attachments/biochem.pdf) 


**Faculty:** Vaishak Belle 

**Postdoctoral fellows and PhD students:**


*   _Paulius Dilkas_, interested in logical abstractions
*   _Miguel Mendez Lucero_, interested in causality
*   _Jonathan Feldstein_ (with James Cheney), interested in probabilistic programming
*   _Fazl Barez_ (with Ekaterina Komendantskaya), interested in explainable AI
*   _Giannis Papantonis_, interested in causality
*   _Ionela-Georgiana Mocanu_, interested in PAC learning
*   _Gary Smith_ (with Ron Petrick), interested in epistemic planning
*   _Andreas Bueff_, interested in tractable learning and reinforcement learning
*   _Sandor Bartha_ (with James Cheney), interested in program induction
*   _Benedicte Legastelois_ (Postdoctoral fellow with TAS project collaborators), interested in explainability 
*   _Xin Du_ (Postdoctoral fellow with TAS project collaborators), interested in explainability 


**Alumni:**

*   _Samuel Kolb_ (PhD 2019, KU Leuven, with Luc De Raedt), interested in inference for hybrid domains
*   _Am√©lie Levray_ (Postdoctoral fellow 2018-2019), interested in tractable learning with credal networks
*   _Davide Nitti_ (PhD 2016, KU Leuven, with Luc De Raedt), interested in machine learning for hybrid domains
*   _Eleanor Platt_ (research associate), interested in explainability 
*   MSc students, including _Stefanie Speichert_ (ILP in hybrid domains), _Laszlo Treszkai_ (generalized planning), _Lewis Hammond_ (moral responsibility), _Michael Varley_ (Fairness)
*   _Amit Parag_ (MscR 2019), interested in tractable models and cosmological simulations  
*   _Rafael Karampatsis_ (Postdoctoral fellow 2019-2021), interested in ML interpretability 

**Visitors:**

*   _Esra Erdem_, Sabanci University
*   _Yoram Moses_, Technion
*   _Brendan Juba_, Washington University in St. Louis
*   _Loizos Michael_ (via the Alan Turing Institute), Open University of Cyprus
*   _Till Hoffman,_ RWTH Aachen University
