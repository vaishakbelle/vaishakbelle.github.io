---
layout: page
title: Belle Lab
permalink: /lab/
---

**TL;DR:** Here's a one-slide summary of our recent work:

<img src="/slide.jpg" width="450"> 

---



The Lab carries out research in **artificial intelligence**, by unifying **learning** and **reasoning**, with a recent emphasis on _neuro-symbolic AI_, _causality_, *explainability* and *ethics*. 

<img src="/uni.png" width="450"> 

We are motivated by the need to augment _learning_ and _perception_ with _high-level structured, commonsensical knowledge_, to enable systems to learn faster and more accurate models of the world. We are interested in developing computational frameworks that are able to _explain their decisions, modular, re-usable_, and _robust_ to variations in problem description. Here is a slightly more granular view of the research that we do, where  neurosymbolics AI sits at the intersection of learning and reasoning + our emphasis  on expressive languages.

<img src="/nesy.jpg" width="450">  


A non-exhaustive list of topics include:

*   probabilistic and statistical knowledge bases
*   ethics and explainability in AI 
*   exact and approximate probabilistic inference
*   statistical relational learning and causality
*   unifying deep learning and probabilistic learning methods
*   probabilistic programming
*   numerical optimization
*   automated planning and high-level programming
*   reinforcement learning and learning for automated planning
*   cognitive robotics
*   automated reasoning
*   modal logics (knowledge, action, belief)
*   multi-agent systems and epistemic planning
*   integrating causality and learning 

For example, our recent work has touched upon: 

*   [morality in machine learning systems](https://arxiv.org/pdf/1810.03736) 
*   [tractable learning with relational logic](/attachments/pacfol.pdf) 
*   [deep tractable probabilistic generative models](https://arxiv.org/pdf/1807.05464) 
*   [learning with missing data](https://arxiv.org/pdf/1901.05847)
*   [program learning for explainability](/attachments/ilp2019.pdf) 
*   [implementing fairness](https://arxiv.org/abs/1905.07026)
*   [model abstraction for explainability](https://arxiv.org/pdf/1810.02434) 
*   [strategies for interpretable & responsible AI](/attachments/biochem.pdf) 


**Faculty:** Vaishak Belle 

**Postdoctoral fellows and PhD students:**



*   _Antonio Miceli_ (Postdoctoral fellow with AISI), interested in large language models and games 
*   _Jessica Ciupa_ (DAIR CDT), interested in RL and ethics 
*   _Nijesh Uperti_, interested in abstraction and logic-based learning
*   _Ruta Tang_, interested in large language models and logic
*  _Ambrose Brown_, interested in explanations and LLMs
*  _Max Scribner_ (Masters by Research), interested in Neuro-symbolic AI and language
  



**Alumni:**

*   _Daga Panas_ (Postdoctoral fellow with Cisco), interested in large language models, logic and vision
*   _Antonio Miceli_ (Postdoctoral fellow with Cisco), interested in large language models and programs 
*   _Xue Li_ (Postdoctoral fellow with Bjorn Ross), interested in counterfactuals and language models
*   _Daxin Liu_ (Postdoctoral fellow with Royal Society), interested in probabilistic modal logics
*   _Miguel Mendez Lucero_ (PhD 2024), interested in neuro-symbolic AI
*  _Chris Sypherd_ (Masters by Research 2025), interested in agents and LLMs
*   _Jessica Ciupa_ (Masters by Research 2024), interested in RL and ethics 
*   _Benedicte Legastelois_ (Postdoctoral fellow with TAS project collaborators), interested in explainability
*   _Jonathan Feldstein_ (PhD 2024), interested in neuro-symbolic AI
*   _Andreas Bueff_ (PhD 2023), interested in tractable learning and reinforcement learning
*   _Giannis Papantonis_ (PhD 2023), interested in causality
*   _Ionela-Georgiana Mocanu_ (PhD 2023), interested in PAC learning
*   _Paulius Dilkas_ (PhD 2022), interested in model counting 
*   _Xin Du_ (Postdoctoral fellow with TAS project collaborators), interested in explainability 
*   _Am√©lie Levray_ (Postdoctoral fellow 2018-2019), interested in tractable learning with credal networks
*   _Eleanor Platt_ (research associate), interested in explainability 
*   _Amit Parag_ (MscR 2019), interested in tractable models and cosmological simulations  
*   _Rafael Karampatsis_ (Postdoctoral fellow 2019-2021), interested in ML interpretability 

**Associates:** 

*   _Sandor Bartha_ (with James Cheney, PhD 2023), interested in program induction
*   _Gary Smith_ (with Ron Petrick), interested in epistemic planning
*   _Eddie Ungless_ (with Bjorn Ross), interested in NLP and bias
*   _Samuel Kolb_ (PhD 2019, KU Leuven, with Luc De Raedt), interested in inference for hybrid domains
*   _Davide Nitti_ (PhD 2016, KU Leuven, with Luc De Raedt), interested in machine learning for hybrid domains





**Visitors:**

*   _Esra Erdem_, Sabanci University
*   _Yoram Moses_, Technion
*   _Brendan Juba_, Washington University in St. Louis
*   _Loizos Michael_ (via the Alan Turing Institute), Open University of Cyprus
*   _Till Hoffman,_ RWTH Aachen University
*   _Xenia Heilmann_, Mainz University
*   _Chiara Manganini_, University of Milan 


**MSc Students:** 

If you are an Informatics MSc student at the University of Edinburgh, I supervise a number of theses on above topics. See, for example, the [publications](/papers) of other MSc students, including _Stefanie Speichert_ (ILP in hybrid domains), _Laszlo Treszkai_ (generalized planning), _Lewis Hammond_ (moral responsibility), and _Michael Varley_ (Fairness).
